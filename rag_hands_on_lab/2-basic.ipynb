{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85450d30-4f49-405e-97ec-b3eee8d315bc",
   "metadata": {},
   "source": [
    "# Gen AI 기반 대화형 챗봇 구현 (약 30분 소요)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9052a7a1-183f-48b4-b2f4-95cf31ad7af9",
   "metadata": {},
   "source": [
    "### Bedrock 클라이언트 설정 및 연결 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5588305e-15eb-4c39-9c4b-da7de8e9cd5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3\n",
    "from botocore.config import Config\n",
    "from pprint import pprint\n",
    "\n",
    "session = boto3.Session()\n",
    "region_name = 'us-east-1'\n",
    "\n",
    "retry_config = Config(\n",
    "        region_name=region_name,\n",
    "        retries={\n",
    "            \"max_attempts\": 10,\n",
    "            \"mode\": \"standard\",\n",
    "        },\n",
    "    )\n",
    "# Bedrock 클라이언트 설정\n",
    "bedrock = boto3.client(\"bedrock\", region_name=region_name, config=retry_config)\n",
    "bedrock_runtime = session.client(\"bedrock-runtime\", region_name=region_name, config=retry_config)\n",
    "\n",
    "# # 사용 가능한 Foundation 모델 목록 조회\n",
    "# model_list = bedrock.list_foundation_models()\n",
    "# result = [(fm[\"modelName\"], fm[\"modelId\"]) for fm in model_list[\"modelSummaries\"] if 'ON_DEMAND' in fm['inferenceTypesSupported']]\n",
    "# pprint(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26526943-c923-4037-b266-c584222eb76a",
   "metadata": {},
   "source": [
    "### 응답의 출력 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1bc25267-0dd4-49d4-b76f-6d09b4cc1656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실시간 스트리밍으로 제공되는 Response의 delta 값을 즉시 출력하기 위한 모듈\n",
    "def bedrock_streamer(response):\n",
    "    stream = response.get('body')\n",
    "    answer = \"\"\n",
    "    i = 1\n",
    "    if stream:\n",
    "        for event in stream:\n",
    "            chunk = event.get('chunk')\n",
    "            if  chunk:\n",
    "                chunk_obj = json.loads(chunk.get('bytes').decode())\n",
    "                if \"delta\" in chunk_obj:                    \n",
    "                    delta = chunk_obj['delta']\n",
    "                    if \"text\" in delta:\n",
    "                        text=delta['text'] \n",
    "                        print(text, end=\"\")\n",
    "                        answer+=str(text)       \n",
    "                        i+=1\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff45151-58d5-4f18-ab09-7f447e0f0cdf",
   "metadata": {},
   "source": [
    "### 입력 형식 정의\n",
    "\n",
    "프롬프트를 모델의 입력 템플릿에 맞춰 저장합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "676bbe4c-bcf8-460e-92d1-cba738bed755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 프롬프트 기본 구성 정의\n",
    "prompt_data = \"한국 리테일/CPG 산업의 동향에 대해 자세히 알려주세요\"\n",
    "\n",
    "prompt_config = {\n",
    "    \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "    \"max_tokens\": 4096,\n",
    "    \"temperature\" : 0,\n",
    "    \"top_k\": 350,\n",
    "    \"top_p\": 0.999,\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": prompt_data},\n",
    "            ],\n",
    "        }\n",
    "    ],\n",
    "}\n",
    "\n",
    "body = json.dumps(prompt_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f52a8b-8126-476f-9321-c69fbefa5475",
   "metadata": {},
   "source": [
    "### 모델 호출 - `invoke_model_with_response_stream()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a187798-c1a3-4236-82c0-2c1b1990f907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "한국의 리테일/CPG(소비재) 산업은 최근 몇 가지 주요 동향을 보이고 있습니다.\n",
      "\n",
      "1. 온라인 쇼핑 증가\n",
      "코로나19 팬데믹으로 인해 비대면 소비가 크게 늘어나면서 온라인 쇼핑 시장이 급성장했습니다. 특히 식료품, 생활용품 등의 온라인 구매가 큰 폭으로 증가했습니다. 이에 따라 기업들은 온라인 판매 채널 강화에 주력하고 있습니다.\n",
      "\n",
      "2. 편의점 시장 경쟁 심화\n",
      "편의점 시장이 포화 상태에 이르면서 기존 주요 편의점 브랜드 간 경쟁이 치열해지고 있습니다. 상품 다양화, 배달서비스 강화 등 차별화 전략을 내세우고 있습니다.\n",
      "\n",
      "3. 프리미엄/HMR 제품 인기\n",
      "소비자들의 웰빙 트렌드와 더불어 간편하고 고급스러운 제품에 대한 수요가 높아지고 있습니다. 프리미엄 식품, HMR(가정간편식) 제품 등이 인기를 끌고 있습니다.\n",
      "\n",
      "4. 친환경/지속가능 제품 관심 증가  \n",
      "환경과 지속가능성에 대한 관심이 높아지면서 친환경 포장, 재활용 원료 사용 등 지속가능한 제품을 선호하는 추세입니다.\n",
      "\n",
      "5. 기술 활용 강화\n",
      "인공지능, 빅데이터 등 신기술을 활용해 소비자 니즈 파악, 상품기획, 물류 효율화 등을 도모하는 기업들이 늘고 있습니다.\n",
      "\n",
      "이처럼 한국 리테일/CPG 시장은 디지털 전환, 프리미엄화, 친환경 트렌드 등 다양한 변화를 겪고 있으며, 기업들은 이에 발맞춰 혁신을 모색하고 있습니다."
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output, display, display_markdown, Markdown\n",
    "\n",
    "modelId = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "accept = \"application/json\"\n",
    "contentType = \"application/json\"\n",
    "\n",
    "try:\n",
    "\n",
    "    response = bedrock_runtime.invoke_model_with_response_stream(\n",
    "        body=body, modelId=modelId, accept=accept, contentType=contentType\n",
    "    )\n",
    "    results=bedrock_streamer(response)\n",
    "\n",
    "except botocore.exceptions.ClientError as error:\n",
    "\n",
    "    if error.response['Error']['Code'] == 'AccessDeniedException':\n",
    "           print(f\"\\x1b[41m{error.response['Error']['Message']}\\\n",
    "                \\nTo troubeshoot this issue please refer to the following resources.\\\n",
    "                 \\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/troubleshoot_access-denied.html\\\n",
    "                 \\nhttps://docs.aws.amazon.com/bedrock/latest/userguide/security-iam.html\\x1b[0m\\n\")\n",
    "\n",
    "    else:\n",
    "        raise error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86cc61e-51bc-4de0-b275-1c411f9cfb43",
   "metadata": {},
   "source": [
    "### 챗봇에 기본 대화 기능 추가\n",
    "위에서 우리는 Gen AI 모델과 상호작용 하기 위한 기본 대화기능을 실습했습니다.\n",
    "\n",
    "아래에서는 이 대화 기능을 Streamlit 애플리케이션의 챗봇에 반영합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8cd99c6f-85e8-4f58-b114-9c445eebddd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting basic.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile basic.py\n",
    "import boto3\n",
    "import json\n",
    "import streamlit as st\n",
    "from botocore.config import Config\n",
    "from langchain.callbacks.base import BaseCallbackHandler\n",
    "\n",
    "###### Bedrock 클라이언트 생성 부분 ######\n",
    "session = boto3.Session()\n",
    "bedrock_runtime = session.client(\n",
    "    service_name=\"bedrock-runtime\",\n",
    "    config=Config(\n",
    "        region_name='us-east-1',\n",
    "        retries={\"max_attempts\": 10, \"mode\": \"standard\"}\n",
    "    )\n",
    ")\n",
    "\n",
    "###### 스트리밍 응답 처리 ######\n",
    "class StreamHandler(BaseCallbackHandler):\n",
    "    def __init__(self, placeholder):\n",
    "        super().__init__()\n",
    "        self.placeholder = placeholder\n",
    "        self.accumulated_text = \"\"\n",
    "\n",
    "    def on_llm_new_token(self, token: str, **kwargs):\n",
    "        self.accumulated_text += token\n",
    "        self.placeholder.text(self.accumulated_text)\n",
    "\n",
    "###### 메시지 처리 ######\n",
    "def search_basic(prompt, chat_box):    \n",
    "    prompt_config = {\n",
    "        \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "        \"max_tokens\": 4096,\n",
    "        \"temperature\": 0,\n",
    "        \"top_k\": 350,\n",
    "        \"top_p\": 0.999,\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "    }\n",
    "    body = json.dumps(prompt_config)\n",
    "    stream_handler = StreamHandler(chat_box)\n",
    "    \n",
    "    try:\n",
    "        ###### Bedrock 호출 부분 ######\n",
    "        response = bedrock_runtime.invoke_model_with_response_stream(\n",
    "            body=body, modelId=\"anthropic.claude-3-sonnet-20240229-v1:0\",\n",
    "            accept=\"application/json\", contentType=\"application/json\"\n",
    "        )\n",
    "        for event in response.get('body'):\n",
    "            chunk = event.get('chunk')\n",
    "            if chunk:\n",
    "                chunk_obj = json.loads(chunk.get('bytes').decode())\n",
    "                delta = chunk_obj.get('delta')\n",
    "                if delta and \"text\" in delta:\n",
    "                    text = delta['text']\n",
    "                    stream_handler.on_llm_new_token(text)\n",
    "    except Exception as error:\n",
    "        st.error(f\"Error: {str(error)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "346d45fa-7032-4337-8c22-0859a2dae1a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting demo-app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile demo-app.py\n",
    "from basic import search_basic  # 수정된 부분\n",
    "import streamlit as st\n",
    "\n",
    "uploaded_file = st.file_uploader(\"검색에 사용할 파일을 업로드하세요\", type=[\"pdf\", \"docx\", \"pptx\"], accept_multiple_files=False)\n",
    "prompt = st.text_input(\"프롬프트를 입력하세요.\")\n",
    "search_type = st.radio(\"Search Type\", [\"Basic\", \"Basic-RAG\", \"Hybrid-RAG\", \"Advanced-RAG\"])\n",
    "\n",
    "chat_box = st.empty()\n",
    "\n",
    "def search_documents(search_type: str, prompt: str, file, chat_box):\n",
    "    if search_type == \"Basic\":\n",
    "        search_basic(prompt, chat_box)   # 수정된 부분\n",
    "    elif search_type == \"Basic-RAG\":\n",
    "        st.write(f\"문서 기본 검색: {prompt}\")\n",
    "    elif search_type == \"Hybrid-RAG\":\n",
    "        st.write(f\"하이브리드 검색: {prompt}\")\n",
    "    elif search_type == \"Advanced-RAG\":\n",
    "        st.write(f\"고급 검색: {prompt}\")\n",
    "\n",
    "if st.button(\"검색\"):\n",
    "    search_documents(search_type, prompt, uploaded_file, chat_box)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4ace6b-96b2-42c1-ab2c-e66fa6451b2b",
   "metadata": {},
   "source": [
    "### 같은 질문을 Streamlit 애플리케이션에 입력\n",
    "\"한국 리테일/CPG 산업의 동향에 대해 알려주세요\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fca91ad-5209-4ddc-86cb-b68862f4a65e",
   "metadata": {},
   "source": [
    "<img src=\"./image/basic-1.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "042d9da9-f095-46c0-9110-afcf5a7887c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting basic.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile basic.py\n",
    "import boto3\n",
    "import json\n",
    "import streamlit as st\n",
    "from botocore.config import Config\n",
    "from langchain.callbacks.base import BaseCallbackHandler\n",
    "\n",
    "###### Bedrock 클라이언트 생성 부분 ######\n",
    "session = boto3.Session()\n",
    "bedrock_runtime = session.client(\n",
    "    service_name=\"bedrock-runtime\",\n",
    "    config=Config(\n",
    "        region_name='us-east-1',\n",
    "        retries={\"max_attempts\": 10, \"mode\": \"standard\"}\n",
    "    )\n",
    ")\n",
    "\n",
    "###### 스트리밍 응답 처리 ######\n",
    "class StreamHandler(BaseCallbackHandler):\n",
    "    def __init__(self, placeholder):\n",
    "        super().__init__()\n",
    "        self.placeholder = placeholder\n",
    "        self.accumulated_text = \"\"\n",
    "\n",
    "    def on_llm_new_token(self, token: str, **kwargs):\n",
    "        self.accumulated_text += token\n",
    "        self.placeholder.text(self.accumulated_text)\n",
    "\n",
    "###### 메시지 처리 ######\n",
    "def search_basic(prompt, chat_box):    \n",
    "    prompt_config = {\n",
    "        \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "        \"max_tokens\": 4096,\n",
    "        \"temperature\": 0,\n",
    "        \"top_k\": 350,\n",
    "        \"top_p\": 0.999,\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "    }\n",
    "    body = json.dumps(prompt_config)\n",
    "    stream_handler = StreamHandler(chat_box)\n",
    "    \n",
    "    try:\n",
    "        ###### Bedrock 호출 부분 ######\n",
    "        response = bedrock_runtime.invoke_model_with_response_stream(\n",
    "            body=body, modelId=\"anthropic.claude-3-sonnet-20240229-v1:0\",\n",
    "            accept=\"application/json\", contentType=\"application/json\"\n",
    "        )\n",
    "        for event in response.get('body'):\n",
    "            chunk = event.get('chunk')\n",
    "            if chunk:\n",
    "                chunk_obj = json.loads(chunk.get('bytes').decode())\n",
    "                delta = chunk_obj.get('delta')\n",
    "                if delta and \"text\" in delta:\n",
    "                    text = delta['text']\n",
    "                    stream_handler.on_llm_new_token(text)\n",
    "    except Exception as error:\n",
    "        st.error(f\"Error: {str(error)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57bf429c-86a8-4acb-8e6f-8b0722e82bae",
   "metadata": {},
   "source": [
    "하지만 현재 애플리케이션에서는 대화의 문맥이 유지되지 않습니다.\n",
    "\n",
    "예를 들어, \"한 문장으로 요약해주세요\"라고 요청했을 때, 챗봇은 문맥에 맞지않는 답변을 제공할 것입니다.\n",
    "\n",
    "#### 아래에서는 채팅 애플리케이션 설계를 위한 Langchain 활용방법을 알아봅니다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f43559-715d-4939-9596-972f323be912",
   "metadata": {},
   "source": [
    "### Langchain 프레임워크 통합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8da19656-81da-4c97-aa53-3b3786008e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Langchain 라이브러리 불러오기\n",
    "from langchain_community.chat_models import BedrockChat\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4e7da388-359a-4994-a62d-0c56f309ecf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "llmchat = BedrockChat(\n",
    "    model_id=modelId,\n",
    "    streaming=True,\n",
    "    region_name=region_name,\n",
    "    callbacks=[StreamingStdOutCallbackHandler()],\n",
    "    model_kwargs={\n",
    "        \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "        \"max_tokens\": 4096,\n",
    "        \"temperature\" : 0,\n",
    "        \"top_k\": 350,\n",
    "        \"top_p\": 0.999\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f357b6ec-1edb-467f-8bc4-dac7a3683aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "    Human: You're an advanced AI assistant. Please provide a short answer about my query.\n",
    "    <context>\n",
    "    {history}\n",
    "    </context>\n",
    "    query - {input}\n",
    "    \n",
    "    Assistant:\n",
    "\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template,\n",
    "    input_variables=['history', 'input']\n",
    ")\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    llm=llmchat, \n",
    "    verbose=False, \n",
    "    memory=ConversationBufferMemory(),\n",
    "    prompt=PROMPT\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3cd5d4-b688-44bc-bbcc-802c9ccfeee2",
   "metadata": {},
   "source": [
    "### `BedrockChat()` 활용\n",
    "Bedrock 세션과 직접 통신할 때는 low level API인 `invoke_model_with_response_stream()`을 사용했습니다.\n",
    "\n",
    "반면, Langchain에서는 `BedrockChat()` Chat Model이 제공하는 high level API인 `ConversationChain`의 `predict()` 메소드를 사용합니다.\n",
    "\n",
    "*위에 정의한 프롬프트 템플릿(prompt_template)에 따라 답변의 양상은 크게 달라집니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a5605a8c-6423-4148-ad2c-84a41d982205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "한국의 리테일/CPG 산업은 최근 몇 가지 주요 동향을 보이고 있습니다.\n",
      "\n",
      "1. 온라인 쇼핑 성장: 코로나19 팬데믹으로 인해 비대면 소비가 증가하면서 온라인 쇼핑 시장이 크게 성장했습니다. \n",
      "\n",
      "2. 옴니채널 전략: 오프라인 매장과 온라인 채널을 연계한 옴니채널 마케팅이 중요해지고 있습니다.\n",
      "\n",
      "3. 친환경/웰니스 트렌드: 환경과 건강에 대한 관심 증가로 친환경, 웰니스 제품 수요가 늘고 있습니다.  \n",
      "\n",
      "4. 고객 경험 중시: 차별화된 고객 경험을 제공하기 위해 매장 환경, 서비스 개선에 주력하고 있습니다.\n",
      "\n",
      "5. 기술 활용: AI, 빅데이터 등 신기술을 활용한 마케팅, 운영 효율화가 이루어지고 있습니다.\n",
      "\n",
      "이와 같은 주요 동향을 반영하여 리테일/CPG 기업들이 전략을 수립하고 있습니다."
     ]
    },
    {
     "data": {
      "text/plain": [
       "'한국의 리테일/CPG 산업은 최근 몇 가지 주요 동향을 보이고 있습니다.\\n\\n1. 온라인 쇼핑 성장: 코로나19 팬데믹으로 인해 비대면 소비가 증가하면서 온라인 쇼핑 시장이 크게 성장했습니다. \\n\\n2. 옴니채널 전략: 오프라인 매장과 온라인 채널을 연계한 옴니채널 마케팅이 중요해지고 있습니다.\\n\\n3. 친환경/웰니스 트렌드: 환경과 건강에 대한 관심 증가로 친환경, 웰니스 제품 수요가 늘고 있습니다.  \\n\\n4. 고객 경험 중시: 차별화된 고객 경험을 제공하기 위해 매장 환경, 서비스 개선에 주력하고 있습니다.\\n\\n5. 기술 활용: AI, 빅데이터 등 신기술을 활용한 마케팅, 운영 효율화가 이루어지고 있습니다.\\n\\n이와 같은 주요 동향을 반영하여 리테일/CPG 기업들이 전략을 수립하고 있습니다.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"한국 리테일/CPG 산업의 동향에 대해 자세히 알려주세요\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9dcf33-a571-4e08-993f-50c0e2a58534",
   "metadata": {},
   "source": [
    "#### 이제 대화의 컨텍스트가 유지되고 있는지 확인해봅니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "28290f81-7053-4cf7-b563-2a8f901852fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "한국 리테일/CPG 산업은 온라인 쇼핑 성장, 옴니채널 전략, 친환경/웰니스 트렌드, 고객 경험 중시, 신기술 활용 등의 주요 동향을 보이며 변화하고 있습니다."
     ]
    },
    {
     "data": {
      "text/plain": [
       "'한국 리테일/CPG 산업은 온라인 쇼핑 성장, 옴니채널 전략, 친환경/웰니스 트렌드, 고객 경험 중시, 신기술 활용 등의 주요 동향을 보이며 변화하고 있습니다.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"한 문장으로 요약해주세요\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6196bf-302f-4db5-be43-0ec3ce62ce76",
   "metadata": {},
   "source": [
    "#### 챗봇을 완성하기 위해 우리는 아래 작업을 진행했습니다.\n",
    "1. Bedrock 기반의 Chat Model 인스턴스 `BedrockChat()`를 생성합니다.\n",
    "2. 대화를 유지하기 위한 `ConversationChain` 생성합니다. `ConversationBufferMemory`에는 대화 내용이 저장됩니다.\n",
    "3. 위 ConversationChain에 predict() 호출로 메시지를 전달하여 스트리밍 응답을 받습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5d859a-9206-406a-93f7-6af789a792a4",
   "metadata": {},
   "source": [
    "### Streamlit 애플리케이션 업데이트\n",
    "\n",
    "이제 위에서 작업했던 내용을 Streamlit 애플리케이션 코드에 반영해봅니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a16449f5-f593-4c0a-872e-458dbbe4b878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting basic.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile basic.py\n",
    "import streamlit as st\n",
    "from langchain_community.chat_models import BedrockChat\n",
    "from langchain.callbacks.base import BaseCallbackHandler\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "region_name = 'us-east-1'\n",
    "modelId = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "\n",
    "###### 스트리밍 응답 처리 ######\n",
    "class StreamHandler(BaseCallbackHandler):\n",
    "    def __init__(self, placeholder):\n",
    "        super().__init__()\n",
    "        self.placeholder = placeholder\n",
    "        self.accumulated_text = \"\"\n",
    "\n",
    "    def reset_accumulated_text(self):\n",
    "        self.accumulated_text = \"\"\n",
    "        self.placeholder.text(self.accumulated_text)\n",
    "\n",
    "    def on_llm_new_token(self, token: str, **kwargs):\n",
    "        self.accumulated_text += token\n",
    "        self.placeholder.text(self.accumulated_text)\n",
    "\n",
    "def get_conversation(chat_box):\n",
    "    stream_handler = StreamHandler(chat_box)\n",
    "    \n",
    "    llmchat = BedrockChat(\n",
    "        model_id=modelId,\n",
    "        streaming=True,\n",
    "        region_name=region_name,\n",
    "        callbacks=[stream_handler], \n",
    "        model_kwargs={\n",
    "            \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "            \"max_tokens\": 4096,\n",
    "            \"temperature\": 0,\n",
    "            \"top_k\": 350,\n",
    "            \"top_p\": 0.999\n",
    "        }\n",
    "    )\n",
    "\n",
    "    prompt_template = \"\"\"\n",
    "    Human: You're an advanced AI assistant. Please provide a short answer about my query.\n",
    "    <context>\n",
    "    {history}\n",
    "    </context>\n",
    "    query - {input}\n",
    "    \n",
    "    Assistant:\n",
    "    \"\"\"\n",
    "\n",
    "    PROMPT = PromptTemplate(\n",
    "        template=prompt_template,\n",
    "        input_variables=['history', 'input']\n",
    "    )\n",
    "\n",
    "    conversation = ConversationChain(\n",
    "        llm=llmchat, \n",
    "        verbose=False, \n",
    "        memory=ConversationBufferMemory(human_prefix=\"Human\", ai_prefix=\"Assistant\"),\n",
    "        prompt=PROMPT\n",
    "    )\n",
    "\n",
    "    return conversation, stream_handler\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a3b5e12c-5e46-478b-870f-bb93d63d0807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting demo-app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile demo-app.py\n",
    "from basic import get_conversation\n",
    "import streamlit as st\n",
    "\n",
    "uploaded_file = st.file_uploader(\"검색에 사용할 파일을 업로드하세요\", type=[\"pdf\", \"docx\", \"pptx\"], accept_multiple_files=False)\n",
    "prompt = st.text_input(\"프롬프트를 입력하세요.\")\n",
    "search_type = st.radio(\"Search Type\", [\"Basic\", \"Basic-RAG\", \"Hybrid-RAG\", \"Advanced-RAG\"])\n",
    "\n",
    "chat_box = st.empty()\n",
    "\n",
    "if 'conversation' not in st.session_state or 'stream_handler' not in st.session_state:  # 수정된 부분 (대화 내용 유지를 위해 streamlit 세션 관리가 필요합니다)\n",
    "    st.session_state.conversation, st.session_state.stream_handler = get_conversation(chat_box)\n",
    "\n",
    "def search_documents(search_type: str, prompt: str):\n",
    "    if search_type == \"Basic\":\n",
    "        st.session_state.stream_handler.reset_accumulated_text()\n",
    "        st.session_state.conversation.predict(input=prompt)\n",
    "    elif search_type == \"Basic-RAG\":\n",
    "        st.write(f\"문서 기본 검색: {prompt}\")\n",
    "    elif search_type == \"Hybrid-RAG\":\n",
    "        st.write(f\"하이브리드 검색: {prompt}\")\n",
    "    elif search_type == \"Advanced-RAG\":\n",
    "        st.write(f\"고급 검색: {prompt}\")\n",
    "\n",
    "if st.button(\"검색\"):\n",
    "    search_documents(search_type, prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd11012-99cb-424f-a144-097f85b8fb7c",
   "metadata": {},
   "source": [
    "#### 이제 챗봇이 과거 대화내용을 잘 기억하고 있습니다.\n",
    "\n",
    "<img src=\"./image/basic-2.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f1fc63-ecfd-4487-9535-b3d5fbb01b26",
   "metadata": {},
   "source": [
    "#### 이제 생성형 AI 기반 대화기능을 갖춘 챗봇이 완성됐습니다. \n",
    "\n",
    "그런데 우리는 이 챗봇을 특정 사내 지식기반 또는 최신 데이터에 특화시키고 싶습니다.\n",
    "\n",
    "#### 챗봇에 검색증강생성(RAG) 기능을 추가하기 위해 `2-basic-rag.ipynb` 노트북으로 이동합니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
